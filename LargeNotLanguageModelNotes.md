# Notes on papers extending LLM approaches to non-language domains

## GATO: [A Generalist Agent](https://arxiv.org/pdf/2205.06175v3.pdf) (2022)

This work trains an LLM on many modalities of data by flattening. Each wordpiece, image patch, continuous sensory input, or output action becomes a token, so all observations and outputs at a given time point become a subsequence of the time series. 

I'm incredibly confused about how/if the tokens for one modality are separated from another. In some cases, it appears that they are, and in others it seems like they aren't? It's unclear whether the choices made were arbitrary, or they tried multiple things and this worked best, as the experiments are  not comparative about these details. It does not seem like there is enough information to really understand what was done, but there is an attempt to implement this here: https://github.com/OrigamiDream/gato (Tensorflow). 

### Data types

**Simulated control tasks**: These are synthetic datasets generated by agents trained using reinforcement learning. The data consists of the observations, actions, and rewards during training of the RL agent. Observations are either direct observations of the state of the agent e.g. positions and angles of simulated robot components, or frames from e.g. Atari games. Actions are either continuous adjustments of the robot position or discrete button pushes in a video game. 

**Language**: MassiveText - English-language text datasets from a variety of sources. 

**Vision and language**: Several datasets consisting of pairs of images and captions. MultiModal MassiveWeb is webpages with both text and images. Several datasets consist of visual question-answer pairs. 

**Robotics** Real robot - observations are camera images and robot state, with actions that are a mix of continuous and discrete. 

### Tokenization

Each time point is represented by a sequence of tokens. If there are text inputs, these come first -- the tokens representing each word-piece observed. These are in the range \[0,32000). Each word-piece token is embedded into a continuous embedding space via a look-up table. 

Images are next tokenized, following ViT. The image is split into 16x16 patches in raster order. These patches are then continuous vectors of length 256, and are normalized to be in the range [-1/4,1/4] for some reason. A single ResNet block is used to embed each image patch separately. 

Discrete values, e.g. joystick button presses, are encoded as numbers in \[0,1200), so I think they overlap with some word-piece tokens. If there are multiple button presses, they are output in some fixed order, as far as I can tell. 

Continuous values are mu-law encoded (log scaling away from 0), then discretized into 1024 bins. These are then shifted to the range \[32000,33024) so that they don't overlap with word-pieces. I do think they overlap with each other, though. 

Between observations and actions, there is a special separator token. 

### Positional encoding

Positional encoding is learnable and added to token embeddings. Image patch positional encodings only seem to encode location in the image. 

The order within the sequence for a given timestep is encoded  with the local observation position encoding. 

As far as I can tell, there is NO positional encoding information about time step?? That seems bizarre.

### Training

Training only penalizes predictions for text or agent actions, not observations via masking. 

Prompting is used to tell the model which task it is solving. During training, for 25% of sequences, a prompt sequence consisting of another episode from the same agent is prepended. Half of these are from the end of the sequence, and half are from a random location. 

The context length is 1024. This seems relatively small for the control tasks, in which the subsequence length for a given time point could be in the hundreds. 

### Deployment

A fixed prompt is used for each task, e.g. the first 1024 tokens of a demonstration. Action vectors are produced one at a time, then decoded into actions at the end of the time step. 

### Experiments

They do some experiments, but no comparisons, and just say it seems okay at a bunch of tasks. They show that bigger models do better. 

### Out-of-distribution tasks

Their context was too small to use prompting, so they fine-tune the model on the held-out task. Here, they actually did some ablation experimeents. I believe they *never* pre-train on data from the held-out task. They experiment with pre-training on all the data vs pre-training only on data from the same domain vs pre-training on data from only the non-control tasks (vision and language tasks, I believe? possibly robotics is included there??), and no pre-training. 

For the cartpole swing task, there seems to be a benefit to pre-training on all data vs just data from the domain for smaller amounts of fine-tuning. Without pre-training on control data, this model does not do well. For other tasks, fine-tuning from scratch does okay.

The only case where pre-training on non-control data seemed to help was for the DMLab task. Since the input here includes images, the image dataset may be helping. 

### Specialist single-domain multi-task agents

They show that their method works for single-domain data. 

### Data sets

#### Simulated control tasks

*[Meta-World](https://meta-world.github.io/)*: Reinforcement learning, multi-task learning. Simulated Sawyer robot arm moving around various things:
"The action space is a 2-tuple consisting of the change in 3D space of the end-effector followed by a normalized torque that the gripper fingers should apply. The actions in this space range between −1 and 1. For all tasks, the robot must either manipulate one object with a variable goal position, or manipulate two objects with a fixed goal position. The observation space is represented as a 6-tuple of the 3D Cartesian positions of the end-effector, a normalized measurement of how open the gripper is, the 3D position of the first object, the quaternion of the first object, the 3D position of the second object, the quaternion of the second object, all of the previous measurements in the environment, and finally the 3D position of the goal. If there is no second object or the goal is not meant to be included in the observation, then the quantities corresponding to them are zeroed
out. The observation space is always 39 dimensional."
"The multi-component reward function R is the combination of a reaching reward, a grasping reward, and a placing reward"

*[BabyAI](https://arxiv.org/abs/1810.08272)*: 
"2D gridworld in which synthetic natural-looking instructions (e.g. 'put the red ball next to the box on your left') require the agent
to navigate the world (including unlocking doors) and move objects to specified location." 
"At each step, the agent receives a 7x7 representation of its field of view (the grid cells in front of it) as well as a Baby Language instruction (textual string)."
Instructions are in a synthetic Baby Language, a small subset of English, language can be specified in Backus-Naur Form. Actions appear to be a discrete set. 

*[DM Control Suite](https://arxiv.org/pdf/1801.00690v1.pdf)*; Continuous control tasks in MuJoCo, ranging from a pendulum to a humanoid walker. States, actions, and observations are all continuous vectors of different dimensionality, depending on the task, and are related to positions, angles, and movements of parts. The simplest (pendulum) task has state dimensionality 2, action dimensionality 1, observation dimensionality 3, while the most complex (humanoid) has state dimensionality 124, action dimensionality 56, observation dimensionality 137. 

*[Arcade Learning Environment](https://arxiv.org/abs/1207.4708)*: classic Atari games. Observations are single game screens (160x210 7-bit pixels), actions are the set of 18 discrete actions defined by a joystick controller.

*[Procgen Benchmark](https://arxiv.org/pdf/1912.01588.pdf)*: procedurally generated game environments, observations are  discrete 15
dimensional action space and produce 64 × 64 × 3. 

*[Modular RL](https://wenlong.page/modular-rl)*: Gym Mujoco, similar to DM Control Suite. 

## [Trajectory Transformers: Offline reinforcement learning as one big sequence modeling problem](https://github.com/JannerM/trajectory-transformer) (2021)

Reinforcement learning can be viewed as a sequence generation problem, in which the goal is to generate sequences of actions of high reward. From offline data, this work learns to generate sequences of states, actions, and rewards using a transformer. It then uses dynamic programming to find sequences of states that optimize a criterion, e.g. maximizing reward, probability. 

As in GATO (this work precedes GATO), states, actions, and reward for a given time point are flattened into subsequences:
$$\tau = (...,s_t^1,...,s_t^N,a_t^1,a_t^M,r_t,...)$$
States, actions, and rewards are continuous, and they try both uniform and quantile-based strategies for binning the data. Each output is discretized into $V$ bins (in the code, the default number of bins is 100), and tokens are shared between different output types, which can only be distinguished based on the positional encoding. Looking at the code, it looks like the positional encoding is learned. According to the code, there is a different weight for action, reward, and value outputs (not sure what values are). It doesn't look like they ignore observations in the loss function. 

During inference, they use beam search to produce a sequence. At each time $t$ they choose the $B$ subsequences that maximize some criterion. For imitation learning, they maximize the probability of the trajectory. For finding a trajectory that achieves a goal, they permute the input to $(s_T, s_1, ..., s_{T-1})$ (I think during both training and inference?). For offline reinforcement learning, they use beam search to maximize the reward. To make this work, they include a prediction of reward-to-go at each time point, the reward received after the given time point. 

Quantitative evaluation is performed for offline reinforcement learning, with the goal of maximizing the reward (not imitating behavior in the dataset). Thus, the transformer is ideally learning the distribution of states, actions, and rewards, and then beam search is used to plan in this distribution. 

## [Decision Transformer: Reinforcement learning via sequence learning](https://arxiv.org/abs/2106.01345) (2021)


