# Notes on papers extending LLM approaches to non-language domains

## GATO: [A Generalist Agent](https://arxiv.org/pdf/2205.06175v3.pdf)

This work trains an LLM on many modalities of data by flattening. Each wordpiece, image patch, continuous sensory input, or output action becomes a token, so all observations and outputs at a given time point become a subsequence of the time series. 

I'm incredibly confused about how/if the tokens for one modality are separated from another. In some cases, it appears that they are, and in others it seems like they aren't? It's unclear whether the choices made were arbitrary, or they tried multiple things and this worked best, as the experiments are  not comparative about these details. It does not seem like there is enough information to really understand what was done, but there is an attempt to implement this here: https://github.com/OrigamiDream/gato (Tensorflow). 

### Data types

**Simulated control tasks**: These are synthetic datasets generated by agents trained using reinforcement learning. The data consists of the states, actions, and rewards (and I assume observations?) during training. 

*[Meta-World](https://meta-world.github.io/)*: Reinforcement learning, multi-task learning. Simulated Sawyer robot arm moving around various things:
"The action space is a 2-tuple consisting of the change in 3D space of the end-effector followed by a normalized torque that the gripper fingers should apply. The actions in this space range between âˆ’1 and 1. For all tasks, the robot must either manipulate one object with a variable goal position, or manipulate two objects with a fixed goal position. The observation space is represented as a 6-tuple of the 3D Cartesian positions of the end-effector, a normalized measurement of how open the gripper is, the 3D position of the first object, the quaternion of the first object, the 3D position of the second object, the quaternion of the second object, all of the previous measurements in the environment, and finally the 3D position of the goal. If there is no second object or the goal is not meant to be included in the observation, then the quantities corresponding to them are zeroed
out. The observation space is always 39 dimensional."
"The multi-component reward function R is the combination of a reaching reward, a grasping reward, and a placing reward"

*[BabyAI](https://arxiv.org/abs/1810.08272)*: 
"2D gridworld in which synthetic natural-looking instructions (e.g. 'put the red ball next to the box on your left') require the agent
to navigate the world (including unlocking doors) and move objects to specified location." 
"At each step, the agent receives a 7x7 representation of its field of view (the grid cells in front of it) as well as a Baby Language instruction (textual string)."
Instructions are in a synthetic Baby Language, a small subset of English, language can be specified in Backus-Naur Form. Actions appear to be a discrete set. 

*[DM Control Suite](https://arxiv.org/pdf/1801.00690v1.pdf)*; Continuous control tasks in MuJoCo, ranging from a pendulum to a humanoid walker. States, actions, and observations are all continuous vectors of different dimensionality, depending on the task, and are related to positions, angles, and movements of parts. The simplest (pendulum) task has state dimensionality 2, action dimensionality 1, observation dimensionality 3, while the most complex (humanoid) has state dimensionality 124, action dimensionality 56, observation dimensionality 137. 

*[Arcade Learning Environment](https://arxiv.org/abs/1207.4708)*: classic Atari games. Observations are single game screens (160x210 7-bit pixels), actions are the set of 18 discrete actions defined by a joystick controller.

*[Procgen Benchmark](https://arxiv.org/pdf/1912.01588.pdf)*: procedurally generated game environments.
